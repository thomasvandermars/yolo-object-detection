import tensorflow as tf
from .pre_processing_tf import random_contrast_brightness_hue
from .pre_processing_tf import flip, random_flip_left_right
from .pre_processing_tf import check_for_duplicate_indices, remove_duplicate_indices
from .pre_processing_tf import ranking_anchors_by_IoU

def translate(img, boxes, alpha, w_to_h):
    """
    Function randomly tranlates images and its corresponding bounding boxes.
    
    :params tf.Tensor img: image
    :params tf.Tensor boxes: bounding boxes [xmin, ymin, xmax, ymax]. Shape = [n_anchors, 4].
    :params tf.Tensor alpha: translate percentage [0.0, 0.5]
    :params tf.Tensor w_to_h: width to height ratio of the original image. Used to adjust randomly generated offsets
                              to prevent objects from becoming to distorted (offsets some of the distortion generated by resizing)
    
    :return tf.Tensor translated_img: adjusted image based on randomly generated translations.
    :return tf.Tensor translated_boxes: adjusted bounding box coordinates based on randomly generated translations.
    """
    
    # set boundaries on allowed translation
    alpha = tf.maximum(tf.minimum(alpha, 0.5), 0.0)
    x_alpha = tf.multiply(alpha, w_to_h) # adjust allowed x-dimension translation for original width-to-height ratio
    y_alpha = tf.divide(alpha, w_to_h) # adjust allowed y-dimension translation for original width-to-height ratio
    
    # generate random offsets based on given translate percentage 
    x_offset = tf.random.uniform(shape = [1,], minval = tf.multiply(-1., x_alpha), maxval = x_alpha, dtype = tf.float32)[0]
    y_offset = tf.random.uniform(shape = [1,], minval = tf.multiply(-1., y_alpha), maxval = y_alpha, dtype = tf.float32)[0]
    
    # scale up offsets to pixels
    img_h = tf.cast(tf.shape(img)[0], tf.float32)
    img_w = tf.cast(tf.shape(img)[1], tf.float32)
    x_offset = tf.round(tf.multiply(x_offset, img_w))
    y_offset = tf.round(tf.multiply(y_offset, img_h))
    
    # 1.) adjust bounding box coordinates based on offsets
    
    # if x offset is positive --> image is shifted to the right
    x_s = tf.cond(tf.greater(x_offset, 0.), 
              lambda: [tf.minimum(boxes[:,0], tf.subtract(tf.subtract(img_w, x_offset), 1.)),
                       tf.minimum(boxes[:,2], tf.subtract(tf.subtract(img_w, x_offset), 1.))], 
              lambda: [boxes[:,0], boxes[:,2]])

    # if y offset is positive --> image is shifted down
    y_s = tf.cond(tf.greater(y_offset, 0.), 
                  lambda: [tf.minimum(boxes[:,1], tf.subtract(tf.subtract(img_h, y_offset), 1.)),
                           tf.minimum(boxes[:,3], tf.subtract(tf.subtract(img_h, y_offset), 1.))], 
                  lambda: [boxes[:,1], boxes[:,3]])

    # if x offset is negative --> image is shifted to the left
    x_s = tf.cond(tf.less(x_offset, 0.), 
                  lambda: [tf.maximum(tf.add(x_s[0], x_offset), 0.),
                           tf.maximum(tf.add(x_s[1], x_offset), 0.)], 
                  lambda: x_s)

    # if y offset is negative --> image is shifted upwards
    y_s = tf.cond(tf.less(y_offset, 0.), 
                  lambda: [tf.maximum(tf.add(y_s[0], y_offset), 0.),
                           tf.maximum(tf.add(y_s[1], y_offset), 0.)], 
                  lambda: y_s)

    # after translation we have new image dimensions
    new_img_w = tf.subtract(img_w, tf.abs(x_offset))
    new_img_h = tf.subtract(img_h, tf.abs(y_offset))
    
    # concatenate new bounding box coordinates and convert to image relative coordinates [0,1]
    translated_boxes = tf.stack([x_s[0], y_s[0], x_s[1], y_s[1]], axis = 1)
    translated_boxes = tf.divide(translated_boxes, [new_img_w, new_img_h, new_img_w, new_img_h])
    
    # 2.) adjust image based on offsets
    img = tf.cond(tf.greater(x_offset, 0.), lambda: img[:,:tf.cast(tf.subtract(img_w, x_offset), tf.int32),:], lambda: img)
    img = tf.cond(tf.greater(y_offset, 0.), lambda: img[:tf.cast(tf.subtract(img_h, y_offset), tf.int32),...], lambda: img)
    img = tf.cond(tf.less(x_offset, 0.), lambda: img[:,tf.cast(tf.multiply(x_offset, -1.), tf.int32):,:], lambda: img)
    img = tf.cond(tf.less(y_offset, 0.), lambda: img[tf.cast(tf.multiply(y_offset, -1.), tf.int32):,...], lambda: img)
    translated_img = img
    
    return translated_img, translated_boxes

def read_data(filename, tar_h, tar_w, channels):
    """
    Function that reads and resizes images & reads the annotations and stores the bounding box information
    within single lines of string, because we have to deal with a variable number of bounding boxes within the image.
    
    :params tf.Tensor filename: image filename
    :params tf.Tensor tar_h: image target height.
    :params tf.Tensor tar_w: image target width.
    :params tf.Tensor channels: number of color channels. 
    
    :return tf.Tensor img: resized image. Shape = [tar_h, tar_w, channels].
    :return tf.Tensor lbl: annotation (each image's annotation is stored in a single line of strings).
    """
    
    # reading in image
    image = tf.io.read_file(filename)
    image = tf.io.decode_image(image, expand_animations = False, channels = channels) # NOTE that decoded image is normalized to [0,1] range
    image = tf.image.convert_image_dtype(image, tf.float32)

    # take image path and construct corresponding .txt annotation file path
    path = tf.strings.split(filename, '/')
    annotation_paths = tf.strings.join([tf.strings.reduce_join(path[:-2], separator = '/'), 'annotations (txt)', tf.strings.regex_replace(path[-1], '.png|.jpg|.jpeg', '.txt')], separator = '/')

    # extract image shape (after augmentations were done)
    img_h = tf.shape(image)[0]
    img_w = tf.shape(image)[1]

    # adjust bounding box coordinates to new dimensions
    label = tf.io.read_file(annotation_paths)
    label = tf.reshape(tf.strings.split(label), [-1, 5]) # reshape to [n_bboxes, 5]
    coor = label[:,1:] # extract bounding box coordinates [xmin, ymin, xmax, ymax]
    coor = tf.divide(tf.strings.to_number(coor, tf.float32), [img_w, img_h, img_w, img_h]) # convert to original image relative
    coor = tf.multiply(coor, [tar_w, tar_h, tar_w, tar_h]) # scale back up to resized image pixels
    
    # reconstruct single string line annotation
    lbl = tf.concat([tf.expand_dims(label[:,0], axis = -1), tf.strings.as_string(coor)], axis = -1)
    lbl = tf.strings.reduce_join(lbl, separator = ' ')
    lbl = tf.strings.join([tf.strings.as_string(img_h), tf.strings.as_string(img_w), lbl], separator = ' ')
    
    # finally we can resize the image
    img = tf.image.resize(image, [tar_h, tar_w])
    
    return img, lbl

def augmentation(img, lbl, tar_h, tar_w, translation = 0.2, contrast = 0.5, brightness = 0.15, hue = 0.05):
    """
    Function that performs the augmentation functionality.
    
    :params tf.Tensor img: image
    :params tf.Tensor lbl: string annotation
    :params tf.Tensor tar_h: image target height.
    :params tf.Tensor tar_w: image target width.
    :params tf.Tensor translation: degree of translation based on which actual translation is generated.
    :params tf.Tensor contrast: maximum adjustment to image contrast
    :params tf.Tensor brightness: maximum adjustment to image brightness 
    :params tf.Tensor hue: maximum adjustment to hue

    :return tf.Tensor img: augmented image. Shape = [tar_h, tar_w, channels].
    :return tf.Tensor lbl: augmented annotation (each image's annotation is stored in a single line of strings).
    """
    
    # split up corresponding bounding box information stored in strings
    lbl_split = tf.strings.split(lbl)
    img_h = tf.strings.to_number(lbl_split[0], tf.float32)
    img_w = tf.strings.to_number(lbl_split[1], tf.float32)
    w_to_h = tf.divide(img_w, img_h) # width-to-height ratio
   
    # extract coordinates & class labels that need adjusting after translation
    label = tf.reshape(lbl_split[2:], [-1, 5])
    
    # extract bounding box coordinates
    coor = tf.strings.to_number(label[:,1:], tf.float32)
    
    ### AUGMENTATION STEPS ###
    
    # 1.) perform the random translation of image and bounding boxes
    img, coor = translate(img = img, boxes = coor, alpha = translation, w_to_h = w_to_h)
    
    # 1.1) resize the translated image back up to the target dimensions & do the same for bbox coordinates
    img = tf.image.resize(img, [tar_h, tar_w])
    coor = tf.multiply(coor, [tar_w, tar_h, tar_w, tar_h])
    
    # 2.) randomly flip horizontally (left to right)
    img, coor = random_flip_left_right(image = img, boxes = coor)
    
    # 3.) randomly adjust contrast, brightness and hue
    img = random_contrast_brightness_hue(image = img, contrast = contrast, brightness = brightness, hue = hue)
    
    ############################
    
    # reconstruct single line annotations
    lbl = tf.concat([tf.expand_dims(label[:,0], axis = -1), tf.strings.as_string(coor)], axis = -1)
    lbl = tf.strings.join([lbl_split[0], lbl_split[1], tf.strings.reduce_join(lbl, separator=' ')], separator=' ')
    
    return img, lbl

def encode_label(img, boxes, grid_h, grid_w, anchors, classes):
    """
    Function that encodes annotations into YOLO label format.
    
    :params tf.Tensor img: image
    :params tf.Tensor boxes: string annotation
    :params tf.Tensor grid_h: number of vertical grids.
    :params tf.Tensor grid_w: number of horizontal grids.
    :params tf.Tensor anchors: anchors.
    :params tf.Tensor classes: class labels.

    :return tf.Tensor img: image. Shape = [tar_h, tar_w, channels].
    :return tf.Tensor lbl: YOLO encoded label. Shape = [GRID_H, GRID_W, ANCHORS, N_CLASSES + 5]
    """
    
    # split up corresponding bounding box information stored in strings
    # extract coordinates & class labels
    label = tf.reshape(tf.strings.split(boxes)[2:], [-1, 5])
    
    # extract bounding box coordinates
    coor = tf.strings.to_number(label[:,1:], tf.float32)
    
    # extract number of class labels and number of bounding boxes
    n_class = tf.size(classes)
    n_bboxes = tf.cast(tf.shape(label)[0], tf.int32)
    
    # extract image shape (after augmentations were done)
    img_h = tf.cast(tf.shape(img)[0], tf.float32)
    img_w = tf.cast(tf.shape(img)[1], tf.float32)
    
    # convert from absolute [xmin, ymin, xmax, ymax] to grid relative [xmid, ymid, width, height]
    w = tf.divide(tf.subtract(coor[:,2], coor[:,0]), img_w) # image relative bounding box widths
    h = tf.divide(tf.subtract(coor[:,3], coor[:,1]), img_h) # image relative bounding box heights
    x = tf.divide(tf.divide(tf.add(coor[:,2], coor[:,0]), 2.), img_w) # image relative bounding box x-midpoint
    y = tf.divide(tf.divide(tf.add(coor[:,3], coor[:,1]), 2.), img_h) # image relative bounding box y-midpoint

    x_box = tf.multiply(x, grid_w) # grid relative bounding box x-midpoint
    y_box = tf.multiply(y, grid_h) # grid relative bounding box y-midpoint
    w_box = tf.multiply(w, grid_w) # grid relative bounding box widths
    h_box = tf.multiply(h, grid_h) # grid relative bounding box widths

    grid_x = tf.cast(x_box, tf.int32) # x grid
    grid_y = tf.cast(y_box, tf.int32) # y grid
    
    # grid relative bounding box dimensions. Shape = [n_bboxes, 2]
    bbox_dims = tf.stack([h_box, w_box], axis = 1)
    
    # rank anchors by IoU with each bounding box output. Shape = [n_anchors, n_bboxes]
    ordered_anchor_indices = ranking_anchors_by_IoU(anchor_dims = tf.cast(anchors, tf.float32), # Shape = [n_anchors, 2]
                                                    bbox_dims = bbox_dims) # Shape = [n_bboxes, 2]
    ordered_anchor_indices = tf.transpose(ordered_anchor_indices) # [n_bboxes, n_anchors]
    
    # construct grid coordinate indices & the "best" anchor box assignment (GRID_H, GRID_W, ANCHOR_i). Shape = [n_bboxes, 3]
    indices = tf.stack([grid_y, grid_x, tf.cast(ordered_anchor_indices[...,0], tf.int32)], axis = 1)
     
    # meta indices for extracting the best anchors assigned to each bounding box
    idx = tf.stack([tf.range(0, tf.shape(ordered_anchor_indices)[0], 1), 
                    tf.cast(tf.zeros(tf.shape(ordered_anchor_indices)[0]), tf.int32)], axis = -1)
    
    # we need to eliminate any overlapping sets of grid-anchor indices
    tracker = tf.constant(0)
    tracker, indices, idx, ordered_anchor_indices = tf.while_loop(check_for_duplicate_indices, 
                                                                  remove_duplicate_indices, 
                                                                  loop_vars = [tracker, indices, idx, ordered_anchor_indices])
    
    # repeat the grid and anchor index sets 6 times (one for each bbox characteristic). Shape = [n_bboxes * 6, 3]
    indices = tf.repeat(indices, repeats = 6, axis = 0)
    
    # location indices for the objects
    cls_locs = tf.cast(tf.argmax(tf.equal(tf.expand_dims(label[:,0], axis = -1), tf.expand_dims(classes, axis = 0)), axis = 1), tf.int32)
    obj_locs = tf.cast(tf.repeat(n_class, n_bboxes), tf.int32)
    x_locs = tf.repeat(tf.cast(tf.add(n_class, 1), tf.int32), n_bboxes)
    y_locs = tf.repeat(tf.cast(tf.add(n_class, 2), tf.int32), n_bboxes)
    w_locs = tf.repeat(tf.cast(tf.add(n_class, 3), tf.int32), n_bboxes)
    h_locs = tf.repeat(tf.cast(tf.add(n_class, 4), tf.int32), n_bboxes)
    
    locs = tf.reshape(tf.stack([cls_locs, obj_locs, x_locs, y_locs, w_locs, h_locs], axis = 1), [-1])
    
    # add final column with indices
    indices = tf.concat([indices, tf.expand_dims(locs, axis = -1)], axis = -1)
    
    # object values to be stored in encoded label
    values = tf.reshape(tf.stack([tf.ones(n_bboxes), tf.ones(n_bboxes), x_box, y_box, w_box, h_box], axis = 1), [-1])
    
    # place the values within a sparse tensor
    s_t = tf.sparse.SparseTensor(indices = tf.cast(indices, tf.int64),
                                 values = values, 
                                 dense_shape = [grid_h, grid_w, tf.shape(anchors)[0], tf.add(tf.shape(classes)[0], 5)])

    # reorder indices
    t = tf.sparse.reorder(s_t)
    
    # sparse to dense tensor
    lbl = tf.sparse.to_dense(t)
    
    return img, lbl